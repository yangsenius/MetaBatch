# MetaBatch

GreatÔºÅ
1.      Model-agnostic meta-learning for fast adaptation of deep networks. In: ICML. (2017) 
2.      Optimization as a model for few-shot learning. In: ICLR. (2017) 
3.      Learning to learn by gradient descent by gradient descent. In: NIPS. (2016) 
4.      Meta-learning with memory-augmented neural networks. In: ICML. (2016) 
5.      Meta-SGD: Learning to learn quickly for few shot learning. arXiv:1707.09835 (2017) 
6.      Continuous adaptation via meta-learning in nonstationary and competitive environments.= In: ICLR. (2018) 
7.      Evolutionary principles in self-referential learning. Diploma thesis, Institut f. Informatik, Tech. Univ. Munich (1987) 
8.      Learning to control fast-weight memories: an alternative to dynamic recurrent networks. Neural Computation (1992) 
9.      Learning to learn using gradient descents. ICANN (2001) 
10.     Learning to learn: introduction and overview. Springer (1998) 
11.     Learning to learn without gradient descent by gradient descent. In: ICML. (2017) 
12.     Learned optimizers that scale and generalize. In: ICML. (2017)
13.     Learning to optimize. In: ICLR. (2017)
14.     Learning to Learn: Model regression networks for easy small sample learning. In: ECCV. (2016) 
15.     Adam: A method for stochastic optimization. In: ICLR. (2015) 
16.     Gradient-based hyperparameter optimization through reversible learning. In: ICML. (2015)
